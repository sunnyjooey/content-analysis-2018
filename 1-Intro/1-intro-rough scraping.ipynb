{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retreiving and Preparing Text for Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "import lucem_illud #pip install git+git://github.com/Computational-Content-Analysis-2018/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "import requests #for http requests\n",
    "import bs4 #called `beautifulsoup4`, an html parser\n",
    "import pandas #gives us DataFrames\n",
    "import docx #reading MS doc files, install as `python-docx`\n",
    "\n",
    "#Stuff for pdfs\n",
    "#Install as `pdfminer2`\n",
    "import pdfminer.pdfinterp\n",
    "import pdfminer.converter\n",
    "import pdfminer.layout\n",
    "import pdfminer.pdfpage\n",
    "\n",
    "#These come with Python\n",
    "import re #for regexs\n",
    "import urllib \n",
    "import io #for making http requests look like files\n",
    "import json #For Tumblr API responses\n",
    "import os.path #For checking if files exist\n",
    "import os #For making directories\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Text (from saved .html files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countryDict = {'Saudi Arabia': [\n",
    "\"saud1.html\",\n",
    "\"saud2.html\",\n",
    "\"saud3.html\",\n",
    "\"saud4.html\",\n",
    "\"saud5.html\",\n",
    "\"saud6.html\",\n",
    "\"saud7.html\",\n",
    "\"saud8.html\",\n",
    "\"saud9.html\",\n",
    "\"saud10.html\",\n",
    "\"saud11.html\",\n",
    "\"saud12.html\",\n",
    "\"saud13.html\",\n",
    "\"saud14.html\",\n",
    "\"saud15.html\"] ,\n",
    "               'China': [\n",
    "\"china1.html\",\n",
    "\"china2.html\",\n",
    "\"china3.html\",\n",
    "\"china4.html\",\n",
    "\"china5.html\",\n",
    "\"china6.html\",\n",
    "\"china7.html\",\n",
    "\"china8.html\",\n",
    "\"china9.html\",\n",
    "\"china10.html\",\n",
    "\"china11.html\",\n",
    "\"china12.html\",\n",
    "\"china13.html\",\n",
    "\"china14.html\",\n",
    "\"china15.html\"],\n",
    "               'US': [\n",
    "\"us01.html\",\n",
    "\"us02.html\",\n",
    "\"us03.html\",\n",
    "\"us04.html\",\n",
    "\"us05.html\",\n",
    "\"us06.html\",\n",
    "\"us07.html\",\n",
    "\"us08.html\",\n",
    "\"us09.html\",\n",
    "\"us10.html\",\n",
    "\"us11.html\",\n",
    "\"us12.html\",\n",
    "\"us13.html\",\n",
    "\"us14.html\"],\n",
    "               'UK': [\n",
    "\"uk01.html\",\n",
    "\"uk02.html\",\n",
    "\"uk03.html\",\n",
    "\"uk04.html\",\n",
    "\"uk05.html\",\n",
    "\"uk06.html\",\n",
    "\"uk07.html\",\n",
    "\"uk08.html\",\n",
    "\"uk09.html\",\n",
    "\"uk10.html\",\n",
    "\"uk11.html\",\n",
    "\"uk12.html\",\n",
    "\"uk13.html\",\n",
    "\"uk14.html\"],\n",
    "               'Australia': [\n",
    "\"au01.html\",\n",
    "\"au02.html\",\n",
    "\"au03.html\",\n",
    "\"au04.html\",\n",
    "\"au05.html\",\n",
    "\"au06.html\",\n",
    "\"au07.html\",\n",
    "\"au08.html\",\n",
    "\"au09.html\",\n",
    "\"au10.html\",\n",
    "\"au11.html\",\n",
    "\"au12.html\",\n",
    "\"au13.html\",\n",
    "\"au14.html\"],\n",
    "               'Kenya': [\n",
    "\"kenya1.html\",\n",
    "\"kenya2.html\",\n",
    "\"kenya3.html\",\n",
    "\"kenya4.html\",\n",
    "\"kenya5.html\",\n",
    "\"kenya6.html\",\n",
    "\"kenya7.html\",\n",
    "\"kenya8.html\",\n",
    "\"kenya9.html\",\n",
    "\"kenya10.html\",\n",
    "\"kenya11.html\",\n",
    "\"kenya12.html\",\n",
    "\"kenya13.html\",\n",
    "\"kenya14.html\",\n",
    "\"kenya15.html\"],\n",
    "               'India': [\n",
    "\"india1.html\",\n",
    "\"india2.html\",\n",
    "\"india3.html\",\n",
    "\"india4.html\",\n",
    "\"india5.html\",\n",
    "\"india6.html\",\n",
    "\"india7.html\",\n",
    "\"india8.html\",\n",
    "\"india9.html\",\n",
    "\"india10.html\",\n",
    "\"india11.html\",\n",
    "\"india12.html\",\n",
    "\"india13.html\",\n",
    "\"india14.html\",\n",
    "\"india15.html\"]}              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loopThruArticles(countryDict):\n",
    "\n",
    "    parsDict = {'region': [], 'source': [], 'title': [], 'text': [], 'date': [], 'author': [], 'section': []}\n",
    "\n",
    "    for key, val in countryDict.items():\n",
    "        for html in val:\n",
    "            parsDict['region'].append(key)\n",
    "            parseArticle(html, parsDict)\n",
    "    \n",
    "    return parsDict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parseArticle(html, parsDict):\n",
    "    soup = bs4.BeautifulSoup(open(html, 'rt', encoding='UTF8'),'lxml')\n",
    "\n",
    "    # title\n",
    "    title0 = soup.find_all('div', {'class' : 'title'})\n",
    "    title1 = title0[0].text.replace('Hide Details', '')\n",
    "    title = re.sub(r'\\<.*\\>', '', title1)\n",
    "    parsDict['title'].append(title)\n",
    "\n",
    "    # text\n",
    "    body = soup.find_all('div', {'class' : 'body'})\n",
    "    text0 = body[0].text\n",
    "    text1 = re.sub(r'\\n', ' ', text0) #Take out new lines\n",
    "    text2 = re.sub(r\"Source- .+\",\"\", text1) #for Times of India\n",
    "    text = re.sub(r\"Â© .+\",\"\", text2) #for allAfrica\n",
    "    parsDict['text'].append(text)\n",
    "    \n",
    "    with open(\"%s.txt\" % html[:-5], \"w\", encoding='UTF-8') as text_file:\n",
    "        text_file.write(text)\n",
    "\n",
    "    # source & date\n",
    "    source = soup.find_all('div', {'class' : 'source'})\n",
    "    date = re.findall(r'(?:January|February|March|April|May|June|July|August|September|October|November|Dececember)\\s\\d{1,2},\\s\\d{4}', source[0].text)\n",
    "    parsDict['date'].append(date)\n",
    "    pat2 = re.compile(r\"(.*?)-\", re.M)\n",
    "    newssource = pat2.findall(source[0].text)\n",
    "    parsDict['source'].append(newssource[0])\n",
    "\n",
    "    meta = soup.find_all('span', {'class' : 'val'})\n",
    "    # author    \n",
    "    author0 = meta[0]\n",
    "    author = re.sub(r'\\<.*\\>', '', author0.text) #Take out <>\n",
    "    parsDict['author'].append(author)\n",
    "\n",
    "    # section\n",
    "    section0 = meta[1]\n",
    "    section = re.sub(r'\\<.*\\>', '', section0.text) #Take out <>\n",
    "    parsDict['section'].append(section)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pandas.DataFrame(loopThruArticles(countryDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv('news.csv', index_label = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
